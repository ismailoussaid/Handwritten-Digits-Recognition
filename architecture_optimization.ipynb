{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "architecture_optimization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5X60vJg3SsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import Deep Learning librairies\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Sampling\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKFGzwdi3aLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data importation\n",
        "train = pd.read_csv('train.csv').drop('label',axis =1)\n",
        "label = pd.read_csv('train.csv')['label']\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Normalization\n",
        "train = train/255\n",
        "test = test/255\n",
        "\n",
        "# Reshape to create images\n",
        "train = train.values.reshape(-1,28,28,1)\n",
        "test = test.values.reshape(-1,28,28,1)\n",
        "\n",
        "# Split the train and the validation set for the fitting\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(train, label, test_size = 0.2, random_state=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAqFmASC3qo7",
        "colab_type": "text"
      },
      "source": [
        "After multiple executions, I noticed that best results are for epochs of 8, 15 and 30. For practical reasons, I choose to work with 8 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf8mW5k13hyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 8 \n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnXIynka4DFk",
        "colab_type": "text"
      },
      "source": [
        "Since I have implemented (apart) many times CNNs with no augmentation, it appears to be less efficient. \n",
        "We have an accuracy lower by ~0,02% in general and on Kaggle. So, the optimization of hyperparameters is realized with Data Augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQBWH_uy4Cb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Manual data augmentation\n",
        "datagen = ImageDataGenerator(rotation_range=10,\n",
        "                            zoom_range=0.2,\n",
        "                            width_shift_range=0.2,\n",
        "                              height_shift_range=0.2)\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54l9EcR75CmD",
        "colab_type": "text"
      },
      "source": [
        "Let's go for a grid search on number of filters and number of nodes of the hidden layer for the CNN architecture I have chosen to work on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vIUVsUU5fu5",
        "colab_type": "text"
      },
      "source": [
        "The method for the Grid Search is clear:\n",
        "\n",
        "* I take n filters for the \n",
        "two first Conv2D, m filters for the two following Conv2D and p nodes for the hidden layer post-Flatten. \n",
        "* We look for the best tradeoff between n/m/p for (n,m) in (32,64) and p in (128,256) as I know that p = 64 is not very efficient and (n,m) taking the value 128 doesn't improve automatically the cross validation accuracy and take too much time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXVXIHAtUTqA",
        "colab_type": "code",
        "outputId": "35216589-6247-428c-d0f7-11f58bfc397b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#SearchGrid\n",
        "\n",
        "first_part = [32,64]\n",
        "second_part = [32,64]\n",
        "third_part = [128,256]\n",
        "RESULT = []\n",
        "\n",
        "#Define CNN:\n",
        "#[Conv2D->relu]*2 with n filters -> MaxPool2D (2,2) -> Dropout -> \n",
        "#[Conv2D->relu]*2 with m filters -> MaxPool2D (2,2) -> Dropout ->\n",
        "#Flatten -> Dense with p nodes -> Dropout -> Out\n",
        "\n",
        "def CNN(n,m,p):\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(filters = n, kernel_size = (5,5),padding = 'Same', \n",
        "                  activation ='relu', input_shape = (28,28,1)))\n",
        "  model.add(Conv2D(filters = n, kernel_size = (5,5),padding = 'Same', \n",
        "                  activation ='relu'))\n",
        "  model.add(MaxPool2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.25)) #to avoid overfitting\n",
        "\n",
        "\n",
        "  model.add(Conv2D(filters = m, kernel_size = (3,3),padding = 'Same', \n",
        "                  activation ='relu'))\n",
        "  model.add(Conv2D(filters = m, kernel_size = (3,3),padding = 'Same', \n",
        "                  activation ='relu'))\n",
        "  model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "  model.add(Dropout(0.5)) #to avoid overfitting\n",
        "\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(p, activation = \"relu\"))\n",
        "  model.add(Dropout(0.7)) #to avoid overfitting\n",
        "\n",
        "  model.add(Dense(10, activation = \"softmax\")) #10 possible outputs because it is a 10-class problem\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer = 'Adam' , loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "  validation_acc = []\n",
        "\n",
        "  for e in range(epochs):\n",
        "\n",
        "      print('Epoch', e)\n",
        "      batches = 0\n",
        "\n",
        "      for x_batch, y_batch in datagen.flow(X_train, Y_train, batch_size=batch_size):\n",
        "\n",
        "          x_train, x_val, y_train, y_val = train_test_split(x_batch, y_batch, test_size = 0.2, random_state=random_seed)\n",
        "          history_CNN_adam_da = model.fit(X_train, Y_train, validation_data = (X_val, Y_val))\n",
        "          validation_acc.append(history_CNN_adam_da.history['val_acc'])\n",
        "          batches += 1\n",
        "\n",
        "          if batches >= 1:\n",
        "              # we need to break the loop by hand because\n",
        "              # the generator loops indefinitely\n",
        "              break\n",
        "  return validation_acc[-1]\n",
        "\n",
        "for f in first_part:\n",
        "  for s in second_part:\n",
        "    for t in third_part:\n",
        "      result = CNN(f,s,t)\n",
        "      print( \"For the hyparameters: \" + str(f) + '/' + str(s) + '/' + str(t) + ', we have: ' + str( result ) )\n",
        "      RESULT.append(\"For the hyparameters: \" + str(f) + '/' + str(s) + '/' + str(t) + ', we have: ' + str( result ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 215s 6ms/sample - loss: 0.4546 - acc: 0.8535 - val_loss: 0.0883 - val_acc: 0.9745\n",
            "Epoch 1\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 214s 6ms/sample - loss: 0.1800 - acc: 0.9483 - val_loss: 0.0662 - val_acc: 0.9798\n",
            "Epoch 2\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 210s 6ms/sample - loss: 0.1338 - acc: 0.9607 - val_loss: 0.0548 - val_acc: 0.9837\n",
            "Epoch 3\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 210s 6ms/sample - loss: 0.1211 - acc: 0.9662 - val_loss: 0.0469 - val_acc: 0.9852\n",
            "Epoch 4\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 214s 6ms/sample - loss: 0.1067 - acc: 0.9692 - val_loss: 0.0455 - val_acc: 0.9867\n",
            "Epoch 5\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 212s 6ms/sample - loss: 0.0974 - acc: 0.9727 - val_loss: 0.0425 - val_acc: 0.9877\n",
            "Epoch 6\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 211s 6ms/sample - loss: 0.0894 - acc: 0.9756 - val_loss: 0.0391 - val_acc: 0.9879\n",
            "Epoch 7\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 211s 6ms/sample - loss: 0.0810 - acc: 0.9782 - val_loss: 0.0359 - val_acc: 0.9883\n",
            "For the hyparameters: 32/32/128, we have: [0.98833334]\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 215s 6ms/sample - loss: 0.3559 - acc: 0.8843 - val_loss: 0.0873 - val_acc: 0.9725\n",
            "Epoch 1\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 217s 6ms/sample - loss: 0.1379 - acc: 0.9581 - val_loss: 0.0710 - val_acc: 0.9794\n",
            "Epoch 2\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 215s 6ms/sample - loss: 0.1039 - acc: 0.9684 - val_loss: 0.0498 - val_acc: 0.9843\n",
            "Epoch 3\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 215s 6ms/sample - loss: 0.0878 - acc: 0.9747 - val_loss: 0.0481 - val_acc: 0.9857\n",
            "Epoch 4\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 212s 6ms/sample - loss: 0.0784 - acc: 0.9778 - val_loss: 0.0451 - val_acc: 0.9867\n",
            "Epoch 5\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 214s 6ms/sample - loss: 0.0751 - acc: 0.9782 - val_loss: 0.0398 - val_acc: 0.9887\n",
            "Epoch 6\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 215s 6ms/sample - loss: 0.0688 - acc: 0.9797 - val_loss: 0.0373 - val_acc: 0.9893\n",
            "Epoch 7\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 216s 6ms/sample - loss: 0.0640 - acc: 0.9818 - val_loss: 0.0382 - val_acc: 0.9896\n",
            "For the hyparameters: 32/32/256, we have: [0.98964286]\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 249s 7ms/sample - loss: 0.4023 - acc: 0.8724 - val_loss: 0.0696 - val_acc: 0.9785\n",
            "Epoch 1\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 248s 7ms/sample - loss: 0.1557 - acc: 0.9550 - val_loss: 0.0783 - val_acc: 0.9794\n",
            "Epoch 2\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 246s 7ms/sample - loss: 0.1224 - acc: 0.9666 - val_loss: 0.0520 - val_acc: 0.9852\n",
            "Epoch 3\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 244s 7ms/sample - loss: 0.1005 - acc: 0.9723 - val_loss: 0.0422 - val_acc: 0.9868\n",
            "Epoch 4\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 245s 7ms/sample - loss: 0.0887 - acc: 0.9751 - val_loss: 0.0416 - val_acc: 0.9889\n",
            "Epoch 5\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 246s 7ms/sample - loss: 0.0862 - acc: 0.9764 - val_loss: 0.0353 - val_acc: 0.9900\n",
            "Epoch 6\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 249s 7ms/sample - loss: 0.0807 - acc: 0.9780 - val_loss: 0.0342 - val_acc: 0.9906\n",
            "Epoch 7\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 249s 7ms/sample - loss: 0.0719 - acc: 0.9799 - val_loss: 0.0412 - val_acc: 0.9889\n",
            "For the hyparameters: 32/64/128, we have: [0.98892856]\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 252s 8ms/sample - loss: 0.3270 - acc: 0.8965 - val_loss: 0.0726 - val_acc: 0.9769\n",
            "Epoch 1\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 249s 7ms/sample - loss: 0.1193 - acc: 0.9660 - val_loss: 0.0629 - val_acc: 0.9817\n",
            "Epoch 2\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 248s 7ms/sample - loss: 0.0947 - acc: 0.9733 - val_loss: 0.0424 - val_acc: 0.9871\n",
            "Epoch 3\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 248s 7ms/sample - loss: 0.0814 - acc: 0.9776 - val_loss: 0.0396 - val_acc: 0.9886\n",
            "Epoch 4\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 251s 7ms/sample - loss: 0.0701 - acc: 0.9800 - val_loss: 0.0539 - val_acc: 0.9852\n",
            "Epoch 5\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 253s 8ms/sample - loss: 0.0632 - acc: 0.9823 - val_loss: 0.0502 - val_acc: 0.9870\n",
            "Epoch 6\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 254s 8ms/sample - loss: 0.0602 - acc: 0.9821 - val_loss: 0.0323 - val_acc: 0.9900\n",
            "Epoch 7\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 251s 7ms/sample - loss: 0.0564 - acc: 0.9834 - val_loss: 0.0351 - val_acc: 0.9898\n",
            "For the hyparameters: 32/64/256, we have: [0.9897619]\n",
            "Epoch 0\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 496s 15ms/sample - loss: 0.4104 - acc: 0.8701 - val_loss: 0.0765 - val_acc: 0.9757\n",
            "Epoch 1\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 496s 15ms/sample - loss: 0.1601 - acc: 0.9556 - val_loss: 0.0605 - val_acc: 0.9813\n",
            "Epoch 2\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 504s 15ms/sample - loss: 0.1270 - acc: 0.9654 - val_loss: 0.0480 - val_acc: 0.9845\n",
            "Epoch 3\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 499s 15ms/sample - loss: 0.1102 - acc: 0.9701 - val_loss: 0.0367 - val_acc: 0.9890\n",
            "Epoch 4\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 494s 15ms/sample - loss: 0.0959 - acc: 0.9728 - val_loss: 0.0517 - val_acc: 0.9856\n",
            "Epoch 5\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 496s 15ms/sample - loss: 0.0887 - acc: 0.9747 - val_loss: 0.0464 - val_acc: 0.9861\n",
            "Epoch 6\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 497s 15ms/sample - loss: 0.0807 - acc: 0.9782 - val_loss: 0.0383 - val_acc: 0.9889\n",
            "Epoch 7\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 493s 15ms/sample - loss: 0.0713 - acc: 0.9793 - val_loss: 0.0388 - val_acc: 0.9888\n",
            "For the hyparameters: 64/32/128, we have: [0.9888095]\n",
            "Epoch 0\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 496s 15ms/sample - loss: 0.3401 - acc: 0.8890 - val_loss: 0.0802 - val_acc: 0.9751\n",
            "Epoch 1\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 501s 15ms/sample - loss: 0.1277 - acc: 0.9620 - val_loss: 0.0723 - val_acc: 0.9761\n",
            "Epoch 2\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 504s 15ms/sample - loss: 0.1007 - acc: 0.9712 - val_loss: 0.0500 - val_acc: 0.9857\n",
            "Epoch 3\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 492s 15ms/sample - loss: 0.0890 - acc: 0.9744 - val_loss: 0.0410 - val_acc: 0.9864\n",
            "Epoch 4\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 492s 15ms/sample - loss: 0.0765 - acc: 0.9787 - val_loss: 0.0418 - val_acc: 0.9877\n",
            "Epoch 5\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 499s 15ms/sample - loss: 0.0707 - acc: 0.9790 - val_loss: 0.0482 - val_acc: 0.9856\n",
            "Epoch 6\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 495s 15ms/sample - loss: 0.0647 - acc: 0.9812 - val_loss: 0.0415 - val_acc: 0.9883\n",
            "Epoch 7\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 485s 14ms/sample - loss: 0.0612 - acc: 0.9818 - val_loss: 0.0349 - val_acc: 0.9890\n",
            "For the hyparameters: 64/32/256, we have: [0.98904765]\n",
            "Epoch 0\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 538s 16ms/sample - loss: 0.4075 - acc: 0.8704 - val_loss: 0.0760 - val_acc: 0.9768\n",
            "Epoch 1\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 546s 16ms/sample - loss: 0.1563 - acc: 0.9562 - val_loss: 0.0511 - val_acc: 0.9833\n",
            "Epoch 2\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 522s 16ms/sample - loss: 0.1225 - acc: 0.9648 - val_loss: 0.0463 - val_acc: 0.9861\n",
            "Epoch 3\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 518s 15ms/sample - loss: 0.0992 - acc: 0.9715 - val_loss: 0.0452 - val_acc: 0.9874\n",
            "Epoch 4\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 531s 16ms/sample - loss: 0.0939 - acc: 0.9741 - val_loss: 0.0523 - val_acc: 0.9858\n",
            "Epoch 5\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 535s 16ms/sample - loss: 0.0810 - acc: 0.9775 - val_loss: 0.0362 - val_acc: 0.9900\n",
            "Epoch 6\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 540s 16ms/sample - loss: 0.0741 - acc: 0.9789 - val_loss: 0.0359 - val_acc: 0.9906\n",
            "Epoch 7\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 548s 16ms/sample - loss: 0.0693 - acc: 0.9808 - val_loss: 0.0325 - val_acc: 0.9910\n",
            "For the hyparameters: 64/64/128, we have: [0.9909524]\n",
            "Epoch 0\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 553s 16ms/sample - loss: 0.3026 - acc: 0.9033 - val_loss: 0.0663 - val_acc: 0.9788\n",
            "Epoch 1\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 546s 16ms/sample - loss: 0.1105 - acc: 0.9685 - val_loss: 0.0567 - val_acc: 0.9835\n",
            "Epoch 2\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 553s 16ms/sample - loss: 0.0884 - acc: 0.9744 - val_loss: 0.0541 - val_acc: 0.9848\n",
            "Epoch 3\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 556s 17ms/sample - loss: 0.0741 - acc: 0.9777 - val_loss: 0.0716 - val_acc: 0.9829\n",
            "Epoch 4\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 555s 17ms/sample - loss: 0.0643 - acc: 0.9819 - val_loss: 0.0351 - val_acc: 0.9889\n",
            "Epoch 5\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 552s 16ms/sample - loss: 0.0574 - acc: 0.9837 - val_loss: 0.0385 - val_acc: 0.9904\n",
            "Epoch 6\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 554s 16ms/sample - loss: 0.0559 - acc: 0.9835 - val_loss: 0.0378 - val_acc: 0.9887\n",
            "Epoch 7\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "33600/33600 [==============================] - 546s 16ms/sample - loss: 0.0550 - acc: 0.9839 - val_loss: 0.0296 - val_acc: 0.9907\n",
            "For the hyparameters: 64/64/256, we have: [0.9907143]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDehsuz5ek4s",
        "colab_type": "code",
        "outputId": "f9bf836d-343d-4eab-d026-8e7e63f8bc97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "for RES in RESULT:\n",
        "  print(RES)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For the hyparameters: 32/32/128, we have: [0.98833334]\n",
            "For the hyparameters: 32/32/256, we have: [0.98964286]\n",
            "For the hyparameters: 32/64/128, we have: [0.98892856]\n",
            "For the hyparameters: 32/64/256, we have: [0.9897619]\n",
            "For the hyparameters: 64/32/128, we have: [0.9888095]\n",
            "For the hyparameters: 64/32/256, we have: [0.98904765]\n",
            "For the hyparameters: 64/64/128, we have: [0.9909524]\n",
            "For the hyparameters: 64/64/256, we have: [0.9907143]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}